Some weights of ReRanker were not initialized from the model checkpoint at roberta-base and are newly initialized: ['domain_classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'domain_classifier.out_proj.bias', 'classifier.out_proj.weight', 'domain_classifier.dense.weight', 'classifier.dense.bias', 'domain_classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/raid/kavin-intern-maunendra/anaconda3/envs/wow/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
  0%|                                                                                                                                                           | 0/811 [00:00<?, ?it/s]You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.




























  4%|█████▏                                                                                                                                          | 29/811 [03:55<1:45:27,  8.09s/it]
  4%|█████▎                                                                                                                                          | 30/811 [04:03<1:45:08,  8.08s/it]





























100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊| 579/580 [00:59<00:00,  9.52it/s]






























  7%|██████████▍                                                                                                                                     | 59/811 [09:04<1:44:27,  8.34s/it]
  7%|██████████▋                                                                                                                                     | 60/811 [09:12<1:43:49,  8.29s/it]





























100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊| 579/580 [00:59<00:00,  9.59it/s]














  9%|████████████▉                                                                                                                                   | 73/811 [12:02<1:46:45,  8.68s/it]Traceback (most recent call last):
  File "/raid/kavin-intern-maunendra/GenRet/roberta.py", line 255, in <module>
  File "/raid/kavin-intern-maunendra/anaconda3/envs/wow/lib/python3.10/site-packages/transformers/trainer.py", line 1539, in train
    return inner_training_loop(
  File "/raid/kavin-intern-maunendra/anaconda3/envs/wow/lib/python3.10/site-packages/transformers/trainer.py", line 1787, in _inner_training_loop
    for step, inputs in enumerate(epoch_iterator):
  File "/raid/kavin-intern-maunendra/anaconda3/envs/wow/lib/python3.10/site-packages/accelerate/data_loader.py", line 394, in __iter__
    next_batch = next(dataloader_iter)
  File "/raid/kavin-intern-maunendra/anaconda3/envs/wow/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 633, in __next__
    data = self._next_data()
  File "/raid/kavin-intern-maunendra/anaconda3/envs/wow/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 677, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/raid/kavin-intern-maunendra/anaconda3/envs/wow/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 54, in fetch
    return self.collate_fn(data)
  File "/raid/kavin-intern-maunendra/anaconda3/envs/wow/lib/python3.10/site-packages/transformers/data/data_collator.py", line 249, in __call__
    batch = self.tokenizer.pad(
  File "/raid/kavin-intern-maunendra/anaconda3/envs/wow/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 3074, in pad
    return BatchEncoding(batch_outputs, tensor_type=return_tensors)
  File "/raid/kavin-intern-maunendra/anaconda3/envs/wow/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 211, in __init__
    self.convert_to_tensors(tensor_type=tensor_type, prepend_batch_axis=prepend_batch_axis)
  File "/raid/kavin-intern-maunendra/anaconda3/envs/wow/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 731, in convert_to_tensors
    tensor = as_tensor(value)
KeyboardInterrupt