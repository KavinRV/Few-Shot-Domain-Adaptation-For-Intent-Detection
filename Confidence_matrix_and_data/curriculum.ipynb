{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionResetError\u001b[0m                      Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/wow/lib/python3.10/site-packages/urllib3/connectionpool.py:714\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    713\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 714\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    715\u001b[0m     conn,\n\u001b[1;32m    716\u001b[0m     method,\n\u001b[1;32m    717\u001b[0m     url,\n\u001b[1;32m    718\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    719\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    720\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    721\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    722\u001b[0m )\n\u001b[1;32m    724\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    725\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    726\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    727\u001b[0m \u001b[39m# mess.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/wow/lib/python3.10/site-packages/urllib3/connectionpool.py:403\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_conn(conn)\n\u001b[1;32m    404\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    405\u001b[0m     \u001b[39m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/wow/lib/python3.10/site-packages/urllib3/connectionpool.py:1053\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1052\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mgetattr\u001b[39m(conn, \u001b[39m\"\u001b[39m\u001b[39msock\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):  \u001b[39m# AppEngine might not have  `.sock`\u001b[39;00m\n\u001b[0;32m-> 1053\u001b[0m     conn\u001b[39m.\u001b[39;49mconnect()\n\u001b[1;32m   1055\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m conn\u001b[39m.\u001b[39mis_verified:\n",
      "File \u001b[0;32m~/anaconda3/envs/wow/lib/python3.10/site-packages/urllib3/connection.py:419\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    417\u001b[0m     context\u001b[39m.\u001b[39mload_default_certs()\n\u001b[0;32m--> 419\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock \u001b[39m=\u001b[39m ssl_wrap_socket(\n\u001b[1;32m    420\u001b[0m     sock\u001b[39m=\u001b[39;49mconn,\n\u001b[1;32m    421\u001b[0m     keyfile\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkey_file,\n\u001b[1;32m    422\u001b[0m     certfile\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcert_file,\n\u001b[1;32m    423\u001b[0m     key_password\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkey_password,\n\u001b[1;32m    424\u001b[0m     ca_certs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mca_certs,\n\u001b[1;32m    425\u001b[0m     ca_cert_dir\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mca_cert_dir,\n\u001b[1;32m    426\u001b[0m     ca_cert_data\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mca_cert_data,\n\u001b[1;32m    427\u001b[0m     server_hostname\u001b[39m=\u001b[39;49mserver_hostname,\n\u001b[1;32m    428\u001b[0m     ssl_context\u001b[39m=\u001b[39;49mcontext,\n\u001b[1;32m    429\u001b[0m     tls_in_tls\u001b[39m=\u001b[39;49mtls_in_tls,\n\u001b[1;32m    430\u001b[0m )\n\u001b[1;32m    432\u001b[0m \u001b[39m# If we're using all defaults and the connection\u001b[39;00m\n\u001b[1;32m    433\u001b[0m \u001b[39m# is TLSv1 or TLSv1.1 we throw a DeprecationWarning\u001b[39;00m\n\u001b[1;32m    434\u001b[0m \u001b[39m# for the host.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/wow/lib/python3.10/site-packages/urllib3/util/ssl_.py:449\u001b[0m, in \u001b[0;36mssl_wrap_socket\u001b[0;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[39mif\u001b[39;00m send_sni:\n\u001b[0;32m--> 449\u001b[0m     ssl_sock \u001b[39m=\u001b[39m _ssl_wrap_socket_impl(\n\u001b[1;32m    450\u001b[0m         sock, context, tls_in_tls, server_hostname\u001b[39m=\u001b[39;49mserver_hostname\n\u001b[1;32m    451\u001b[0m     )\n\u001b[1;32m    452\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/wow/lib/python3.10/site-packages/urllib3/util/ssl_.py:493\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_impl\u001b[0;34m(sock, ssl_context, tls_in_tls, server_hostname)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[39mif\u001b[39;00m server_hostname:\n\u001b[0;32m--> 493\u001b[0m     \u001b[39mreturn\u001b[39;00m ssl_context\u001b[39m.\u001b[39;49mwrap_socket(sock, server_hostname\u001b[39m=\u001b[39;49mserver_hostname)\n\u001b[1;32m    494\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/wow/lib/python3.10/ssl.py:513\u001b[0m, in \u001b[0;36mSSLContext.wrap_socket\u001b[0;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrap_socket\u001b[39m(\u001b[39mself\u001b[39m, sock, server_side\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    508\u001b[0m                 do_handshake_on_connect\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    509\u001b[0m                 suppress_ragged_eofs\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    510\u001b[0m                 server_hostname\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, session\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    511\u001b[0m     \u001b[39m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[1;32m    512\u001b[0m     \u001b[39m# ctx._wrap_socket()\u001b[39;00m\n\u001b[0;32m--> 513\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msslsocket_class\u001b[39m.\u001b[39;49m_create(\n\u001b[1;32m    514\u001b[0m         sock\u001b[39m=\u001b[39;49msock,\n\u001b[1;32m    515\u001b[0m         server_side\u001b[39m=\u001b[39;49mserver_side,\n\u001b[1;32m    516\u001b[0m         do_handshake_on_connect\u001b[39m=\u001b[39;49mdo_handshake_on_connect,\n\u001b[1;32m    517\u001b[0m         suppress_ragged_eofs\u001b[39m=\u001b[39;49msuppress_ragged_eofs,\n\u001b[1;32m    518\u001b[0m         server_hostname\u001b[39m=\u001b[39;49mserver_hostname,\n\u001b[1;32m    519\u001b[0m         context\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m    520\u001b[0m         session\u001b[39m=\u001b[39;49msession\n\u001b[1;32m    521\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/wow/lib/python3.10/ssl.py:1104\u001b[0m, in \u001b[0;36mSSLSocket._create\u001b[0;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[1;32m   1103\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1104\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdo_handshake()\n\u001b[1;32m   1105\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mOSError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n",
      "File \u001b[0;32m~/anaconda3/envs/wow/lib/python3.10/ssl.py:1375\u001b[0m, in \u001b[0;36mSSLSocket.do_handshake\u001b[0;34m(self, block)\u001b[0m\n\u001b[1;32m   1374\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msettimeout(\u001b[39mNone\u001b[39;00m)\n\u001b[0;32m-> 1375\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mdo_handshake()\n\u001b[1;32m   1376\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "\u001b[0;31mConnectionResetError\u001b[0m: [Errno 104] Connection reset by peer",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mProtocolError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/wow/lib/python3.10/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    487\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    488\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    489\u001b[0m         body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[1;32m    490\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    491\u001b[0m         redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    492\u001b[0m         assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    493\u001b[0m         preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    494\u001b[0m         decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    495\u001b[0m         retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    496\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    497\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    498\u001b[0m     )\n\u001b[1;32m    500\u001b[0m \u001b[39mexcept\u001b[39;00m (ProtocolError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/anaconda3/envs/wow/lib/python3.10/site-packages/urllib3/connectionpool.py:798\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    796\u001b[0m     e \u001b[39m=\u001b[39m ProtocolError(\u001b[39m\"\u001b[39m\u001b[39mConnection aborted.\u001b[39m\u001b[39m\"\u001b[39m, e)\n\u001b[0;32m--> 798\u001b[0m retries \u001b[39m=\u001b[39m retries\u001b[39m.\u001b[39;49mincrement(\n\u001b[1;32m    799\u001b[0m     method, url, error\u001b[39m=\u001b[39;49me, _pool\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m, _stacktrace\u001b[39m=\u001b[39;49msys\u001b[39m.\u001b[39;49mexc_info()[\u001b[39m2\u001b[39;49m]\n\u001b[1;32m    800\u001b[0m )\n\u001b[1;32m    801\u001b[0m retries\u001b[39m.\u001b[39msleep()\n",
      "File \u001b[0;32m~/anaconda3/envs/wow/lib/python3.10/site-packages/urllib3/util/retry.py:550\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    549\u001b[0m \u001b[39mif\u001b[39;00m read \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_method_retryable(method):\n\u001b[0;32m--> 550\u001b[0m     \u001b[39mraise\u001b[39;00m six\u001b[39m.\u001b[39;49mreraise(\u001b[39mtype\u001b[39;49m(error), error, _stacktrace)\n\u001b[1;32m    551\u001b[0m \u001b[39melif\u001b[39;00m read \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/wow/lib/python3.10/site-packages/urllib3/packages/six.py:769\u001b[0m, in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    768\u001b[0m \u001b[39mif\u001b[39;00m value\u001b[39m.\u001b[39m__traceback__ \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m tb:\n\u001b[0;32m--> 769\u001b[0m     \u001b[39mraise\u001b[39;00m value\u001b[39m.\u001b[39mwith_traceback(tb)\n\u001b[1;32m    770\u001b[0m \u001b[39mraise\u001b[39;00m value\n",
      "File \u001b[0;32m~/anaconda3/envs/wow/lib/python3.10/site-packages/urllib3/connectionpool.py:714\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    713\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 714\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    715\u001b[0m     conn,\n\u001b[1;32m    716\u001b[0m     method,\n\u001b[1;32m    717\u001b[0m     url,\n\u001b[1;32m    718\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    719\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    720\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    721\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    722\u001b[0m )\n\u001b[1;32m    724\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    725\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    726\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    727\u001b[0m \u001b[39m# mess.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/wow/lib/python3.10/site-packages/urllib3/connectionpool.py:403\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_conn(conn)\n\u001b[1;32m    404\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    405\u001b[0m     \u001b[39m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/wow/lib/python3.10/site-packages/urllib3/connectionpool.py:1053\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1052\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mgetattr\u001b[39m(conn, \u001b[39m\"\u001b[39m\u001b[39msock\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):  \u001b[39m# AppEngine might not have  `.sock`\u001b[39;00m\n\u001b[0;32m-> 1053\u001b[0m     conn\u001b[39m.\u001b[39;49mconnect()\n\u001b[1;32m   1055\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m conn\u001b[39m.\u001b[39mis_verified:\n",
      "File \u001b[0;32m~/anaconda3/envs/wow/lib/python3.10/site-packages/urllib3/connection.py:419\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    417\u001b[0m     context\u001b[39m.\u001b[39mload_default_certs()\n\u001b[0;32m--> 419\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock \u001b[39m=\u001b[39m ssl_wrap_socket(\n\u001b[1;32m    420\u001b[0m     sock\u001b[39m=\u001b[39;49mconn,\n\u001b[1;32m    421\u001b[0m     keyfile\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkey_file,\n\u001b[1;32m    422\u001b[0m     certfile\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcert_file,\n\u001b[1;32m    423\u001b[0m     key_password\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkey_password,\n\u001b[1;32m    424\u001b[0m     ca_certs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mca_certs,\n\u001b[1;32m    425\u001b[0m     ca_cert_dir\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mca_cert_dir,\n\u001b[1;32m    426\u001b[0m     ca_cert_data\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mca_cert_data,\n\u001b[1;32m    427\u001b[0m     server_hostname\u001b[39m=\u001b[39;49mserver_hostname,\n\u001b[1;32m    428\u001b[0m     ssl_context\u001b[39m=\u001b[39;49mcontext,\n\u001b[1;32m    429\u001b[0m     tls_in_tls\u001b[39m=\u001b[39;49mtls_in_tls,\n\u001b[1;32m    430\u001b[0m )\n\u001b[1;32m    432\u001b[0m \u001b[39m# If we're using all defaults and the connection\u001b[39;00m\n\u001b[1;32m    433\u001b[0m \u001b[39m# is TLSv1 or TLSv1.1 we throw a DeprecationWarning\u001b[39;00m\n\u001b[1;32m    434\u001b[0m \u001b[39m# for the host.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/wow/lib/python3.10/site-packages/urllib3/util/ssl_.py:449\u001b[0m, in \u001b[0;36mssl_wrap_socket\u001b[0;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[39mif\u001b[39;00m send_sni:\n\u001b[0;32m--> 449\u001b[0m     ssl_sock \u001b[39m=\u001b[39m _ssl_wrap_socket_impl(\n\u001b[1;32m    450\u001b[0m         sock, context, tls_in_tls, server_hostname\u001b[39m=\u001b[39;49mserver_hostname\n\u001b[1;32m    451\u001b[0m     )\n\u001b[1;32m    452\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/wow/lib/python3.10/site-packages/urllib3/util/ssl_.py:493\u001b[0m, in \u001b[0;36m_ssl_wrap_socket_impl\u001b[0;34m(sock, ssl_context, tls_in_tls, server_hostname)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[39mif\u001b[39;00m server_hostname:\n\u001b[0;32m--> 493\u001b[0m     \u001b[39mreturn\u001b[39;00m ssl_context\u001b[39m.\u001b[39;49mwrap_socket(sock, server_hostname\u001b[39m=\u001b[39;49mserver_hostname)\n\u001b[1;32m    494\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/wow/lib/python3.10/ssl.py:513\u001b[0m, in \u001b[0;36mSSLContext.wrap_socket\u001b[0;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrap_socket\u001b[39m(\u001b[39mself\u001b[39m, sock, server_side\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    508\u001b[0m                 do_handshake_on_connect\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    509\u001b[0m                 suppress_ragged_eofs\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    510\u001b[0m                 server_hostname\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, session\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    511\u001b[0m     \u001b[39m# SSLSocket class handles server_hostname encoding before it calls\u001b[39;00m\n\u001b[1;32m    512\u001b[0m     \u001b[39m# ctx._wrap_socket()\u001b[39;00m\n\u001b[0;32m--> 513\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msslsocket_class\u001b[39m.\u001b[39;49m_create(\n\u001b[1;32m    514\u001b[0m         sock\u001b[39m=\u001b[39;49msock,\n\u001b[1;32m    515\u001b[0m         server_side\u001b[39m=\u001b[39;49mserver_side,\n\u001b[1;32m    516\u001b[0m         do_handshake_on_connect\u001b[39m=\u001b[39;49mdo_handshake_on_connect,\n\u001b[1;32m    517\u001b[0m         suppress_ragged_eofs\u001b[39m=\u001b[39;49msuppress_ragged_eofs,\n\u001b[1;32m    518\u001b[0m         server_hostname\u001b[39m=\u001b[39;49mserver_hostname,\n\u001b[1;32m    519\u001b[0m         context\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m    520\u001b[0m         session\u001b[39m=\u001b[39;49msession\n\u001b[1;32m    521\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/wow/lib/python3.10/ssl.py:1104\u001b[0m, in \u001b[0;36mSSLSocket._create\u001b[0;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[1;32m   1103\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mdo_handshake_on_connect should not be specified for non-blocking sockets\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1104\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdo_handshake()\n\u001b[1;32m   1105\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mOSError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n",
      "File \u001b[0;32m~/anaconda3/envs/wow/lib/python3.10/ssl.py:1375\u001b[0m, in \u001b[0;36mSSLSocket.do_handshake\u001b[0;34m(self, block)\u001b[0m\n\u001b[1;32m   1374\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msettimeout(\u001b[39mNone\u001b[39;00m)\n\u001b[0;32m-> 1375\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mdo_handshake()\n\u001b[1;32m   1376\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "\u001b[0;31mProtocolError\u001b[0m: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m/raid/kavin-intern-maunendra/GenRet/curriculum.ipynb Cell 1\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.209.71/raid/kavin-intern-maunendra/GenRet/curriculum.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m load_dataset\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.209.71/raid/kavin-intern-maunendra/GenRet/curriculum.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m massive \u001b[39m=\u001b[39m load_dataset(\u001b[39m\"\u001b[39m\u001b[39mAmazonScience/massive\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39men-US\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B192.168.209.71/raid/kavin-intern-maunendra/GenRet/curriculum.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m banking \u001b[39m=\u001b[39m load_dataset(\u001b[39m\"\u001b[39;49m\u001b[39mbanking77\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/wow/lib/python3.10/site-packages/datasets/load.py:2133\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   2130\u001b[0m try_from_hf_gcs \u001b[39m=\u001b[39m path \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m _PACKAGED_DATASETS_MODULES\n\u001b[1;32m   2132\u001b[0m \u001b[39m# Download and prepare data\u001b[39;00m\n\u001b[0;32m-> 2133\u001b[0m builder_instance\u001b[39m.\u001b[39;49mdownload_and_prepare(\n\u001b[1;32m   2134\u001b[0m     download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[1;32m   2135\u001b[0m     download_mode\u001b[39m=\u001b[39;49mdownload_mode,\n\u001b[1;32m   2136\u001b[0m     verification_mode\u001b[39m=\u001b[39;49mverification_mode,\n\u001b[1;32m   2137\u001b[0m     try_from_hf_gcs\u001b[39m=\u001b[39;49mtry_from_hf_gcs,\n\u001b[1;32m   2138\u001b[0m     num_proc\u001b[39m=\u001b[39;49mnum_proc,\n\u001b[1;32m   2139\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[1;32m   2140\u001b[0m )\n\u001b[1;32m   2142\u001b[0m \u001b[39m# Build dataset for splits\u001b[39;00m\n\u001b[1;32m   2143\u001b[0m keep_in_memory \u001b[39m=\u001b[39m (\n\u001b[1;32m   2144\u001b[0m     keep_in_memory \u001b[39mif\u001b[39;00m keep_in_memory \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m is_small_dataset(builder_instance\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mdataset_size)\n\u001b[1;32m   2145\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/wow/lib/python3.10/site-packages/datasets/builder.py:954\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[0;34m(self, output_dir, download_config, download_mode, verification_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[0m\n\u001b[1;32m    952\u001b[0m     \u001b[39mif\u001b[39;00m num_proc \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    953\u001b[0m         prepare_split_kwargs[\u001b[39m\"\u001b[39m\u001b[39mnum_proc\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m num_proc\n\u001b[0;32m--> 954\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_download_and_prepare(\n\u001b[1;32m    955\u001b[0m         dl_manager\u001b[39m=\u001b[39;49mdl_manager,\n\u001b[1;32m    956\u001b[0m         verification_mode\u001b[39m=\u001b[39;49mverification_mode,\n\u001b[1;32m    957\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mprepare_split_kwargs,\n\u001b[1;32m    958\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mdownload_and_prepare_kwargs,\n\u001b[1;32m    959\u001b[0m     )\n\u001b[1;32m    960\u001b[0m \u001b[39m# Sync info\u001b[39;00m\n\u001b[1;32m    961\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mdataset_size \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(split\u001b[39m.\u001b[39mnum_bytes \u001b[39mfor\u001b[39;00m split \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39msplits\u001b[39m.\u001b[39mvalues())\n",
      "File \u001b[0;32m~/anaconda3/envs/wow/lib/python3.10/site-packages/datasets/builder.py:1717\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._download_and_prepare\u001b[0;34m(self, dl_manager, verification_mode, **prepare_splits_kwargs)\u001b[0m\n\u001b[1;32m   1716\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_download_and_prepare\u001b[39m(\u001b[39mself\u001b[39m, dl_manager, verification_mode, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mprepare_splits_kwargs):\n\u001b[0;32m-> 1717\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m_download_and_prepare(\n\u001b[1;32m   1718\u001b[0m         dl_manager,\n\u001b[1;32m   1719\u001b[0m         verification_mode,\n\u001b[1;32m   1720\u001b[0m         check_duplicate_keys\u001b[39m=\u001b[39;49mverification_mode \u001b[39m==\u001b[39;49m VerificationMode\u001b[39m.\u001b[39;49mBASIC_CHECKS\n\u001b[1;32m   1721\u001b[0m         \u001b[39mor\u001b[39;49;00m verification_mode \u001b[39m==\u001b[39;49m VerificationMode\u001b[39m.\u001b[39;49mALL_CHECKS,\n\u001b[1;32m   1722\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mprepare_splits_kwargs,\n\u001b[1;32m   1723\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/wow/lib/python3.10/site-packages/datasets/builder.py:1027\u001b[0m, in \u001b[0;36mDatasetBuilder._download_and_prepare\u001b[0;34m(self, dl_manager, verification_mode, **prepare_split_kwargs)\u001b[0m\n\u001b[1;32m   1025\u001b[0m split_dict \u001b[39m=\u001b[39m SplitDict(dataset_name\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset_name)\n\u001b[1;32m   1026\u001b[0m split_generators_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_split_generators_kwargs(prepare_split_kwargs)\n\u001b[0;32m-> 1027\u001b[0m split_generators \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_split_generators(dl_manager, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msplit_generators_kwargs)\n\u001b[1;32m   1029\u001b[0m \u001b[39m# Checksums verification\u001b[39;00m\n\u001b[1;32m   1030\u001b[0m \u001b[39mif\u001b[39;00m verification_mode \u001b[39m==\u001b[39m VerificationMode\u001b[39m.\u001b[39mALL_CHECKS \u001b[39mand\u001b[39;00m dl_manager\u001b[39m.\u001b[39mrecord_checksums:\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/datasets_modules/datasets/banking77/9898c11f6afa9521953d2ef205667b527bad14ef9cab445d470f16240c8c8ec4/banking77.py:161\u001b[0m, in \u001b[0;36mBanking77._split_generators\u001b[0;34m(self, dl_manager)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_split_generators\u001b[39m(\u001b[39mself\u001b[39m, dl_manager):\n\u001b[1;32m    160\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Returns SplitGenerators.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 161\u001b[0m     train_path \u001b[39m=\u001b[39m dl_manager\u001b[39m.\u001b[39;49mdownload(_TRAIN_DOWNLOAD_URL)\n\u001b[1;32m    162\u001b[0m     test_path \u001b[39m=\u001b[39m dl_manager\u001b[39m.\u001b[39mdownload(_TEST_DOWNLOAD_URL)\n\u001b[1;32m    163\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[1;32m    164\u001b[0m         datasets\u001b[39m.\u001b[39mSplitGenerator(name\u001b[39m=\u001b[39mdatasets\u001b[39m.\u001b[39mSplit\u001b[39m.\u001b[39mTRAIN, gen_kwargs\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mfilepath\u001b[39m\u001b[39m\"\u001b[39m: train_path}),\n\u001b[1;32m    165\u001b[0m         datasets\u001b[39m.\u001b[39mSplitGenerator(name\u001b[39m=\u001b[39mdatasets\u001b[39m.\u001b[39mSplit\u001b[39m.\u001b[39mTEST, gen_kwargs\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mfilepath\u001b[39m\u001b[39m\"\u001b[39m: test_path}),\n\u001b[1;32m    166\u001b[0m     ]\n",
      "File \u001b[0;32m~/anaconda3/envs/wow/lib/python3.10/site-packages/datasets/download/download_manager.py:427\u001b[0m, in \u001b[0;36mDownloadManager.download\u001b[0;34m(self, url_or_urls)\u001b[0m\n\u001b[1;32m    424\u001b[0m download_func \u001b[39m=\u001b[39m partial(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_download, download_config\u001b[39m=\u001b[39mdownload_config)\n\u001b[1;32m    426\u001b[0m start_time \u001b[39m=\u001b[39m datetime\u001b[39m.\u001b[39mnow()\n\u001b[0;32m--> 427\u001b[0m downloaded_path_or_paths \u001b[39m=\u001b[39m map_nested(\n\u001b[1;32m    428\u001b[0m     download_func,\n\u001b[1;32m    429\u001b[0m     url_or_urls,\n\u001b[1;32m    430\u001b[0m     map_tuple\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    431\u001b[0m     num_proc\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mnum_proc,\n\u001b[1;32m    432\u001b[0m     disable_tqdm\u001b[39m=\u001b[39;49m\u001b[39mnot\u001b[39;49;00m is_progress_bar_enabled(),\n\u001b[1;32m    433\u001b[0m     desc\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mDownloading data files\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    434\u001b[0m )\n\u001b[1;32m    435\u001b[0m duration \u001b[39m=\u001b[39m datetime\u001b[39m.\u001b[39mnow() \u001b[39m-\u001b[39m start_time\n\u001b[1;32m    436\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDownloading took \u001b[39m\u001b[39m{\u001b[39;00mduration\u001b[39m.\u001b[39mtotal_seconds()\u001b[39m \u001b[39m\u001b[39m/\u001b[39m\u001b[39m/\u001b[39m\u001b[39m \u001b[39m\u001b[39m60\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m min\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/wow/lib/python3.10/site-packages/datasets/utils/py_utils.py:455\u001b[0m, in \u001b[0;36mmap_nested\u001b[0;34m(function, data_struct, dict_only, map_list, map_tuple, map_numpy, num_proc, parallel_min_length, types, disable_tqdm, desc)\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[39m# Singleton\u001b[39;00m\n\u001b[1;32m    454\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(data_struct, \u001b[39mdict\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(data_struct, types):\n\u001b[0;32m--> 455\u001b[0m     \u001b[39mreturn\u001b[39;00m function(data_struct)\n\u001b[1;32m    457\u001b[0m disable_tqdm \u001b[39m=\u001b[39m disable_tqdm \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m logging\u001b[39m.\u001b[39mis_progress_bar_enabled()\n\u001b[1;32m    458\u001b[0m iterable \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(data_struct\u001b[39m.\u001b[39mvalues()) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_struct, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m data_struct\n",
      "File \u001b[0;32m~/anaconda3/envs/wow/lib/python3.10/site-packages/datasets/download/download_manager.py:453\u001b[0m, in \u001b[0;36mDownloadManager._download\u001b[0;34m(self, url_or_filename, download_config)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[39mif\u001b[39;00m is_relative_path(url_or_filename):\n\u001b[1;32m    451\u001b[0m     \u001b[39m# append the relative path to the base_path\u001b[39;00m\n\u001b[1;32m    452\u001b[0m     url_or_filename \u001b[39m=\u001b[39m url_or_path_join(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_base_path, url_or_filename)\n\u001b[0;32m--> 453\u001b[0m \u001b[39mreturn\u001b[39;00m cached_path(url_or_filename, download_config\u001b[39m=\u001b[39;49mdownload_config)\n",
      "File \u001b[0;32m~/anaconda3/envs/wow/lib/python3.10/site-packages/datasets/utils/file_utils.py:182\u001b[0m, in \u001b[0;36mcached_path\u001b[0;34m(url_or_filename, download_config, **download_kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m     url_or_filename \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(url_or_filename)\n\u001b[1;32m    180\u001b[0m \u001b[39mif\u001b[39;00m is_remote_url(url_or_filename):\n\u001b[1;32m    181\u001b[0m     \u001b[39m# URL, so get it from the cache (downloading if necessary)\u001b[39;00m\n\u001b[0;32m--> 182\u001b[0m     output_path \u001b[39m=\u001b[39m get_from_cache(\n\u001b[1;32m    183\u001b[0m         url_or_filename,\n\u001b[1;32m    184\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    185\u001b[0m         force_download\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mforce_download,\n\u001b[1;32m    186\u001b[0m         proxies\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mproxies,\n\u001b[1;32m    187\u001b[0m         resume_download\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mresume_download,\n\u001b[1;32m    188\u001b[0m         user_agent\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49muser_agent,\n\u001b[1;32m    189\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mlocal_files_only,\n\u001b[1;32m    190\u001b[0m         use_etag\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49muse_etag,\n\u001b[1;32m    191\u001b[0m         max_retries\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    192\u001b[0m         token\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mtoken,\n\u001b[1;32m    193\u001b[0m         ignore_url_params\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mignore_url_params,\n\u001b[1;32m    194\u001b[0m         storage_options\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mstorage_options,\n\u001b[1;32m    195\u001b[0m         download_desc\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mdownload_desc,\n\u001b[1;32m    196\u001b[0m     )\n\u001b[1;32m    197\u001b[0m \u001b[39melif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(url_or_filename):\n\u001b[1;32m    198\u001b[0m     \u001b[39m# File, and it exists.\u001b[39;00m\n\u001b[1;32m    199\u001b[0m     output_path \u001b[39m=\u001b[39m url_or_filename\n",
      "File \u001b[0;32m~/anaconda3/envs/wow/lib/python3.10/site-packages/datasets/utils/file_utils.py:644\u001b[0m, in \u001b[0;36mget_from_cache\u001b[0;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, local_files_only, use_etag, max_retries, token, use_auth_token, ignore_url_params, storage_options, download_desc)\u001b[0m\n\u001b[1;32m    642\u001b[0m         fsspec_get(url, temp_file, storage_options\u001b[39m=\u001b[39mstorage_options, desc\u001b[39m=\u001b[39mdownload_desc)\n\u001b[1;32m    643\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 644\u001b[0m         http_get(\n\u001b[1;32m    645\u001b[0m             url,\n\u001b[1;32m    646\u001b[0m             temp_file,\n\u001b[1;32m    647\u001b[0m             proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    648\u001b[0m             resume_size\u001b[39m=\u001b[39;49mresume_size,\n\u001b[1;32m    649\u001b[0m             headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    650\u001b[0m             cookies\u001b[39m=\u001b[39;49mcookies,\n\u001b[1;32m    651\u001b[0m             max_retries\u001b[39m=\u001b[39;49mmax_retries,\n\u001b[1;32m    652\u001b[0m             desc\u001b[39m=\u001b[39;49mdownload_desc,\n\u001b[1;32m    653\u001b[0m         )\n\u001b[1;32m    655\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mstoring \u001b[39m\u001b[39m{\u001b[39;00murl\u001b[39m}\u001b[39;00m\u001b[39m in cache at \u001b[39m\u001b[39m{\u001b[39;00mcache_path\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    656\u001b[0m shutil\u001b[39m.\u001b[39mmove(temp_file\u001b[39m.\u001b[39mname, cache_path)\n",
      "File \u001b[0;32m~/anaconda3/envs/wow/lib/python3.10/site-packages/datasets/utils/file_utils.py:397\u001b[0m, in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers, cookies, timeout, max_retries, desc)\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[39mif\u001b[39;00m resume_size \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    396\u001b[0m     headers[\u001b[39m\"\u001b[39m\u001b[39mRange\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbytes=\u001b[39m\u001b[39m{\u001b[39;00mresume_size\u001b[39m:\u001b[39;00m\u001b[39md\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m-\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 397\u001b[0m response \u001b[39m=\u001b[39m _request_with_retry(\n\u001b[1;32m    398\u001b[0m     method\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mGET\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    399\u001b[0m     url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    400\u001b[0m     stream\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    401\u001b[0m     proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    402\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    403\u001b[0m     cookies\u001b[39m=\u001b[39;49mcookies,\n\u001b[1;32m    404\u001b[0m     max_retries\u001b[39m=\u001b[39;49mmax_retries,\n\u001b[1;32m    405\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    406\u001b[0m )\n\u001b[1;32m    407\u001b[0m \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39m==\u001b[39m \u001b[39m416\u001b[39m:  \u001b[39m# Range not satisfiable\u001b[39;00m\n\u001b[1;32m    408\u001b[0m     \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/wow/lib/python3.10/site-packages/datasets/utils/file_utils.py:332\u001b[0m, in \u001b[0;36m_request_with_retry\u001b[0;34m(method, url, max_retries, base_wait_time, max_wait_time, timeout, **params)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[39mexcept\u001b[39;00m (requests\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mConnectTimeout, requests\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mConnectionError) \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    331\u001b[0m     \u001b[39mif\u001b[39;00m tries \u001b[39m>\u001b[39m max_retries:\n\u001b[0;32m--> 332\u001b[0m         \u001b[39mraise\u001b[39;00m err\n\u001b[1;32m    333\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    334\u001b[0m         logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmethod\u001b[39m}\u001b[39;00m\u001b[39m request to \u001b[39m\u001b[39m{\u001b[39;00murl\u001b[39m}\u001b[39;00m\u001b[39m timed out, retrying... [\u001b[39m\u001b[39m{\u001b[39;00mtries\u001b[39m/\u001b[39mmax_retries\u001b[39m}\u001b[39;00m\u001b[39m]\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/wow/lib/python3.10/site-packages/datasets/utils/file_utils.py:328\u001b[0m, in \u001b[0;36m_request_with_retry\u001b[0;34m(method, url, max_retries, base_wait_time, max_wait_time, timeout, **params)\u001b[0m\n\u001b[1;32m    326\u001b[0m tries \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    327\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 328\u001b[0m     response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod\u001b[39m.\u001b[39;49mupper(), url\u001b[39m=\u001b[39;49murl, timeout\u001b[39m=\u001b[39;49mtimeout, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams)\n\u001b[1;32m    329\u001b[0m     success \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    330\u001b[0m \u001b[39mexcept\u001b[39;00m (requests\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mConnectTimeout, requests\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mConnectionError) \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/anaconda3/envs/wow/lib/python3.10/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod, url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/wow/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    591\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/anaconda3/envs/wow/lib/python3.10/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    705\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[0;32m~/anaconda3/envs/wow/lib/python3.10/site-packages/requests/adapters.py:501\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    486\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39murlopen(\n\u001b[1;32m    487\u001b[0m         method\u001b[39m=\u001b[39mrequest\u001b[39m.\u001b[39mmethod,\n\u001b[1;32m    488\u001b[0m         url\u001b[39m=\u001b[39murl,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    497\u001b[0m         chunked\u001b[39m=\u001b[39mchunked,\n\u001b[1;32m    498\u001b[0m     )\n\u001b[1;32m    500\u001b[0m \u001b[39mexcept\u001b[39;00m (ProtocolError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m--> 501\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(err, request\u001b[39m=\u001b[39mrequest)\n\u001b[1;32m    503\u001b[0m \u001b[39mexcept\u001b[39;00m MaxRetryError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    504\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e\u001b[39m.\u001b[39mreason, ConnectTimeoutError):\n\u001b[1;32m    505\u001b[0m         \u001b[39m# TODO: Remove this in 3.0.0: see #2811\u001b[39;00m\n",
      "\u001b[0;31mConnectionError\u001b[0m: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "massive = load_dataset(\"AmazonScience/massive\", \"en-US\")\n",
    "banking = load_dataset(\"banking77\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'locale', 'partition', 'scenario', 'intent', 'utt', 'annot_utt', 'worker_id', 'slot_method', 'judgments'],\n",
       "        num_rows: 11514\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'locale', 'partition', 'scenario', 'intent', 'utt', 'annot_utt', 'worker_id', 'slot_method', 'judgments'],\n",
       "        num_rows: 2033\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'locale', 'partition', 'scenario', 'intent', 'utt', 'annot_utt', 'worker_id', 'slot_method', 'judgments'],\n",
       "        num_rows: 2974\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "massive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "sur_df = {\"indoml_id\": [], \"id\": [], \"utt\": [], \"intent\": []}\n",
    "id2ut = {}\n",
    "lst = []\n",
    "\n",
    "# Open the file for reading\n",
    "with open('surprise.data', 'r') as file:\n",
    "    # Iterate through each line in the file\n",
    "    for line in file:\n",
    "        # Parse the JSON data in each line\n",
    "        data = json.loads(line)\n",
    "        assert data[\"id\"] not in id2ut.keys()\n",
    "        lst.append(int(data[\"id\"]))\n",
    "        id2ut[data[\"id\"]] = data[\"utt\"]\n",
    "\n",
    "        for k, v in data.items():\n",
    "            sur_df[k].append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('surprise.solution', 'r') as file:\n",
    "    for i, line in enumerate(file):\n",
    "        data = json.loads(line)\n",
    "        assert lst[i] == int(data[\"indoml_id\"][9:])\n",
    "        # print(data.keys())\n",
    "        sur_df[\"intent\"].append(data[\"intent\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "def dict_to_dataset(dict):\n",
    "    dataset = Dataset.from_dict(dict)\n",
    "    return dataset\n",
    "\n",
    "dataset = dict_to_dataset(sur_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "int2eg_sur = {}\n",
    "dup_int = []\n",
    "\n",
    "for eg in dataset:\n",
    "    try:\n",
    "        if eg[\"utt\"] in int2eg_sur[eg[\"intent\"]]:\n",
    "            # dup_int.append[eg[\"intent\"]]\n",
    "            if eg[\"intent\"] not in dup_int:\n",
    "                dup_int.append(eg[\"intent\"])\n",
    "            continue\n",
    "        int2eg_sur[eg[\"intent\"]].append(eg[\"utt\"])\n",
    "    except:\n",
    "        int2eg_sur[eg[\"intent\"]] = [eg[\"utt\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# samples examples from an intent\n",
    "def few_sample(intent, l):\n",
    "    num = l\n",
    "    few = random.sample(int2eg_sur[intent], num)\n",
    "    random.shuffle(few)\n",
    "    return few"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "int2id_sur = {k:i for i, k in enumerate(int2eg_sur.keys())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Load the confusion matrix from the pickle file\n",
    "with open(\"id2label_dict.pkl\", \"rb\") as file:\n",
    "    id2int = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "int2eg = {}\n",
    "for eg in massive[\"train\"]:\n",
    "    try:\n",
    "        int2eg[id2int[eg[\"intent\"]]].append(eg[\"utt\"])\n",
    "    except:\n",
    "        int2eg[id2int[eg[\"intent\"]]] = [eg[\"utt\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval = {}\n",
    "int2egn = {}\n",
    "for k, v in int2eg.items():\n",
    "    if len(v) <= 70:\n",
    "        eval[k] = v\n",
    "        dup_int.append(k)\n",
    "        continue\n",
    "    int2egn[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['general_greet',\n",
       " 'datetime_convert',\n",
       " 'music_settings',\n",
       " 'music_dislikeness',\n",
       " 'audio_volume_other',\n",
       " 'iot_wemo_on',\n",
       " 'iot_hue_lighton',\n",
       " 'iot_wemo_off',\n",
       " 'audio_volume_down',\n",
       " 'recommendation_movies',\n",
       " 'cooking_query',\n",
       " 'email_addcontact']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dup_int[-12:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eval.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['indoml_id', 'id', 'utt', 'intent'],\n",
       "    num_rows: 2248\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "int2egn2 = {}\n",
    "for k, v in int2egn.items():\n",
    "    l = len(v)\n",
    "    if l > 200:\n",
    "        int2egn2[k] = random.sample(int2egn[k], int(0.1*l))\n",
    "    else:\n",
    "        int2egn2[k] = random.sample(int2egn[k], int(0.15*l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11514/11514 [00:01<00:00, 7471.84it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "mass_trn = {0: {\"intent\": [], \"query\": [], \"domain\": []}, 1: {\"intent\": [], \"query\": [], \"domain\": []}}\n",
    "for eg in tqdm(massive[\"train\"]):\n",
    "    if id2int[eg[\"intent\"]] in dup_int:\n",
    "        # mass_trn[1][\"domain\"].append(0)\n",
    "        # mass_trn[1][\"intent\"].append(id2int[eg[\"intent\"]])\n",
    "        # mass_trn[1][\"query\"].append(eg[\"utt\"])\n",
    "        continue\n",
    "    if eg[\"utt\"] in int2egn2[id2int[eg[\"intent\"]]]:\n",
    "        continue\n",
    "    mass_trn[0][\"domain\"].append(0)\n",
    "    mass_trn[0][\"intent\"].append(id2int[eg[\"intent\"]])\n",
    "    mass_trn[0][\"query\"].append(eg[\"utt\"])\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [33, 43, 2, 12].shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"That's correct!\",\n",
       " \"Absolutely! I couldn't agree more!\",\n",
       " \"Of course! That's exactly what I was thinking.\",\n",
       " \"Agreed, let's make it happen!\",\n",
       " 'you hit the nail on the head!',\n",
       " \"Well said! Couldn't have put it better myself.\",\n",
       " \"Agreed! That's the way to go.\",\n",
       " \"Absolutely, I couldn't agree more!\",\n",
       " 'of course, I understand completely!',\n",
       " \"indeed, that's what I was told!\",\n",
       " \"Oh, without a doubt, that's absolutely true\",\n",
       " \"It's a wonderful notion!\",\n",
       " \"That's a superb solution!\"]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int2eg_sur[\"yes\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "int2egn_sur = {}\n",
    "eval_sur = {}\n",
    "for k, v in int2eg_sur.items():\n",
    "    random.shuffle(v)\n",
    "    # eval_sur[k] = v[:]\n",
    "    int2egn_sur[k] = v[:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for eg in dataset:\n",
    "    mass_trn[1][\"domain\"].append(1)\n",
    "    mass_trn[1][\"intent\"].append(eg[\"intent\"])\n",
    "    mass_trn[1][\"query\"].append(eg[\"utt\"])\n",
    "    mass_trn[0][\"domain\"].append(1)\n",
    "    mass_trn[0][\"intent\"].append(eg[\"intent\"])\n",
    "    mass_trn[0][\"query\"].append(eg[\"utt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = {\"intent\": [], \"query\": [], \"domain\": []}\n",
    "with open(\"massive_test.data\", 'r') as file:\n",
    "    for line in file:\n",
    "        data = json.loads(line)\n",
    "        mass_trn[0][\"query\"].append(data[\"utt\"])\n",
    "        mass_trn[0][\"domain\"].append(2)\n",
    "\n",
    "with open(\"massive_test_xlmr_easy_currep4.predict\") as file:\n",
    "    for line in file:\n",
    "        data = json.loads(line)\n",
    "        mass_trn[0][\"intent\"].append(data[\"intent\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# int2sam = []\n",
    "# for k, v in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Load the confusion matrix from the pickle file\n",
    "with open(\"confidence_matrix.pkl\", \"rb\") as file:\n",
    "    sur_conf = pickle.load(file)\n",
    "\n",
    "with open(\"confidence_matrix_mass.pkl\", \"rb\") as file:\n",
    "    mass_conf = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_inter = dict_to_dataset(mass_trn[1])\n",
    "eval_inter = eval_inter.shuffle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0, 101,  26,  ...,  52, 125,  75],\n",
       "        [  1,  45,   6,  ...,  22,  37, 103],\n",
       "        [  2, 113, 122,  ...,  37, 143, 103],\n",
       "        ...,\n",
       "        [147, 148, 120,  ...,  39,  75,  37],\n",
       "        [148, 147, 145,  ..., 121, 103,  37],\n",
       "        [149,  76,  66,  ...,  39,  36,  37]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.tensor(sur_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, j in enumerate(mass_conf):\n",
    "    assert i in j[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [2, 4, 5, 7].remove(44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 4, 5]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = [1, 3, 4, 5]\n",
    "s = c[:]\n",
    "s.remove(3)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "int2id = {v: k for k, v in id2int.items()}\n",
    "# int2id_sur = id2\n",
    "id2int_sur = {v: k for k, v in int2id_sur.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2248/2248 [00:00<00:00, 3783.37it/s]\n"
     ]
    }
   ],
   "source": [
    "eval_dt = {\"query\": [], \"intent\": [], \"positive\": [], \"negatives\": [], \"domain_label\": []}\n",
    "\n",
    "for eg in tqdm(eval_inter):\n",
    "    if eg[\"domain\"] == 0:\n",
    "        eval_dt[\"query\"].append(eg[\"query\"])\n",
    "        eval_dt[\"domain_label\"].append(0)\n",
    "        conf = mass_conf[int2id[eg[\"intent\"]]][:10]\n",
    "        assert int2id[eg[\"intent\"]] in conf\n",
    "        random.shuffle(conf)\n",
    "        neg = []\n",
    "        inte = []\n",
    "        for i, c in enumerate(conf):\n",
    "            wrd = id2int[c]\n",
    "            if c == int2id[eg[\"intent\"]]:\n",
    "                eval_dt[\"positive\"].append(i)\n",
    "            try:\n",
    "                ths = eval[wrd]\n",
    "            except:\n",
    "                ths = int2egn2[wrd][:]\n",
    "                if eg[\"query\"] in ths:\n",
    "                    ths.remove(eg[\"query\"])\n",
    "            assert random.sample(ths, min(10, len(ths))) != []\n",
    "            neg.append(random.sample(ths, min(10, len(ths))))\n",
    "            inte.append(id2int[c])\n",
    "\n",
    "        eval_dt[\"negatives\"].append(neg)\n",
    "        eval_dt[\"intent\"].append(inte)\n",
    "    \n",
    "    else:\n",
    "        eval_dt[\"query\"].append(eg[\"query\"])\n",
    "        eval_dt[\"domain_label\"].append(1)\n",
    "        conf = sur_conf[int2id_sur[eg[\"intent\"]]][:10]\n",
    "        assert int2id_sur[eg[\"intent\"]] in conf\n",
    "        random.shuffle(conf)\n",
    "        neg = []\n",
    "        inte = []\n",
    "        for i, c in enumerate(conf):\n",
    "            if c == int2id_sur[eg[\"intent\"]]:\n",
    "                eval_dt[\"positive\"].append(i)\n",
    "            try:\n",
    "                ths = int2egn_sur[id2int_sur[c]]\n",
    "                a = 2\n",
    "            except:\n",
    "                a = 1\n",
    "                ths = eval_sur[id2int_sur[c]][:]\n",
    "                if eg[\"query\"] in ths:\n",
    "                    ths.remove(eg[\"query\"])\n",
    "            if random.sample(ths, len(ths)) == []:\n",
    "                print(c)\n",
    "                             \n",
    "\n",
    "            neg.append(random.sample(ths, len(ths)))\n",
    "            inte.append(id2int_sur[c])\n",
    "\n",
    "        eval_dt[\"negatives\"].append(neg)\n",
    "        eval_dt[\"intent\"].append(inte)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_sur[id2int_sur[149]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['query', 'intent', 'positive', 'negatives', 'domain_label'],\n",
       "    num_rows: 2248\n",
       "})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_hf = dict_to_dataset(eval_dt)\n",
    "eval_hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'switching to a different inflection would be great!',\n",
       " 'intent': ['change ai name',\n",
       "  'change user name',\n",
       "  'change language',\n",
       "  'play music',\n",
       "  'change volume',\n",
       "  'whisper mode',\n",
       "  'insurance change',\n",
       "  'are you a bot',\n",
       "  'what is your name',\n",
       "  'change accent'],\n",
       " 'positive': 9,\n",
       " 'negatives': [['Can I start calling you Ethel?',\n",
       "   'Could you kindly alter your appellation to something more appealing?',\n",
       "   'i wish to address you as caroline.',\n",
       "   'I think the name River suits you well.',\n",
       "   'can i suggest a few alternative names for you?',\n",
       "   'I would love to dub thee with a fresh handle!',\n",
       "   'can i call you something different, like ethan?',\n",
       "   'Can I interest you in a name change to \"Astrid\"?',\n",
       "   'how about we rename this ai to something more memorable?',\n",
       "   'would you prefer if i called you brody',\n",
       "   \"i have a feeling that 'zelda' would suit you better.\",\n",
       "   'i think it would be great if we could change your name to something funnier.',\n",
       "   'would you prefer i refer to this ai by a different name?',\n",
       "   'can we give it a more fitting name?',\n",
       "   'May I modify your moniker to something more fitting?'],\n",
       "  [\"I'd be happy with any nickname you choose!\",\n",
       "   \"I'd prefer it if you called me Emily instead of Elizabeth.\",\n",
       "   'From today onwards, I would prefer to be referred to as Ava.',\n",
       "   'Could you kindly update my profile with my new name, Emily?',\n",
       "   \"I'd appreciate it if you could start calling me David.\",\n",
       "   'How about we stick with Emily?',\n",
       "   \"I'd love it if you could start referring to me as Ava instead of Amelia.\",\n",
       "   'I wish you could refer to me as Charlotte instead of Benjamin.',\n",
       "   'you can address me as samantha.',\n",
       "   \"Hi there! I'd like to be referred to as Jaxon moving forward. Thanks!\",\n",
       "   'Would you prefer I call you Bob?',\n",
       "   'My new name is Ruby, please use it moving forward.',\n",
       "   'I would prefer it if you called me Sarah instead of Samantha.',\n",
       "   'Kindly call me Rachel instead of Tom.',\n",
       "   'Some folks call me Steve.'],\n",
       "  ['can you switch to english for this conversation?',\n",
       "   'I would appreciate it if you could change my language to Mandarin Chinese.',\n",
       "   'I would prefer to communicate in Spanish.',\n",
       "   'set your language preference to Turkish.',\n",
       "   'Kindly switch to Japanese so we can communicate more effectively.',\n",
       "   'Would it be possible for you to speak in Arabic?',\n",
       "   'Could you please communicate in French?',\n",
       "   'how do I alter the spoken language?',\n",
       "   'Kindly alter my language preference to Japanese.',\n",
       "   'could we switch to speaking mandarin?',\n",
       "   'Can you communicate with me in Japanese?',\n",
       "   'Can you alter my language settings to French?',\n",
       "   'Can you please change your language to Italian?',\n",
       "   'I would love to hear you speak in Turkish',\n",
       "   'would you mind switching to polish for this interaction?'],\n",
       "  ['play something chill, man. I need to relax.',\n",
       "   \"Can you play some classical music while we eat dinner? It's so elegant and sophisticated\",\n",
       "   'play something that makes me feel alive.',\n",
       "   \"I'm hosting a party tonight and I need some funky tunes to keep the crowd moving. Can you help me out?\",\n",
       "   'can you please play my favorite song on repeat?',\n",
       "   'I would be grateful if you could play some soothing music by Fleet Foxes.',\n",
       "   'I need you to play my favorite song from the 80s.',\n",
       "   'do you have any vinyl records? I want to spin some tunes tonight.',\n",
       "   \"I want to jam out to some classic rock, play Led Zeppelin's greatest hits for me.\",\n",
       "   \"I'm in the mood for some blues.\",\n",
       "   'Can you put on some tunes?',\n",
       "   \"I'm cooking dinner and I want to listen to something light-hearted and fun. Can you suggest some good options?\",\n",
       "   \"I'm feeling nostalgic today, can you put on some old-school tunes from my childhood?\",\n",
       "   'May I have the pleasure of listening to the melody that sounds like bells going off?',\n",
       "   \"It's time to rock out with our instruments!\"],\n",
       "  ['get louder)',\n",
       "   'i would like the volume lower)',\n",
       "   'crank up the stereo.',\n",
       "   'adjust the sound level.',\n",
       "   'I would like you to lower the volume of your voice.',\n",
       "   'please adjust your settings)',\n",
       "   'i need you to increase your speaker volume)',\n",
       "   'increase volume to 4)',\n",
       "   'can you turn this down)',\n",
       "   'boost the audio.',\n",
       "   'raise the volume to 4 please)',\n",
       "   'modify the volume level.',\n",
       "   'Your voice is too soft; could you raise it a bit?',\n",
       "   'I wish there was a way to adjust the volume of your voice.',\n",
       "   'bump up the volume.'],\n",
       "  ['please keep your voice down.',\n",
       "   'quiet down and speak softly.',\n",
       "   'I wish we could communicate quietly without being overheard.',\n",
       "   'Can you refrain from shouting?',\n",
       "   'switch to low volume now.',\n",
       "   'Your whisper is so quiet that I can barely hear you. Could you please speak up or use a louder microphone?',\n",
       "   'i need you to whisper this message.',\n",
       "   \"I'm trying to concentrate; could you please whisper?\",\n",
       "   'I suggest we converse softly.',\n",
       "   'whisper sweet nothings into my ear.',\n",
       "   'I wish we were in a place where we could whisper our secrets without worrying about being overheard.',\n",
       "   'I think we should hush our voices.',\n",
       "   'If you could kindly reduce the volume to a soft whisper, it would be greatly appreciated.',\n",
       "   'engage mute mode immediately.',\n",
       "   \"Could you please speak more softly? It's bothering me.\"],\n",
       "  ['I need to adjust my insurance coverage to match my changing needs.',\n",
       "   \"I'm looking to switch to a new auto insurance provider.\",\n",
       "   'How can I upgrade my current insurance policy?',\n",
       "   'I want to switch to a different auto insurance provider.',\n",
       "   \"I'm interested in learning more about insurance plans available in the market.\",\n",
       "   'I would like to downgrade my insurance plan to save money on premiums while still maintaining adequate coverage.',\n",
       "   'i would like to purchase additional liability insurance.',\n",
       "   'I am considering changing my insurance plan to one that provides more flexible coverage options.',\n",
       "   'I would like to switch to a different health insurance plan that offers better coverage for mental health services.',\n",
       "   'Can you help me compare different auto insurance policies?',\n",
       "   'I would like to switch to a more comprehensive insurance plan that provides additional coverage.',\n",
       "   'I would like to upgrade my current insurance plan to one that offers more comprehensive coverage.',\n",
       "   \"I'm considering switching to a different type of insurance (e.g., from auto to homeowners or renters insurance).\",\n",
       "   'How do I go about updating my insurance policy?',\n",
       "   'How do I go about switching to a new health insurance provider?'],\n",
       "  [\"I'm starting to feel like you're not just a chatbot.\",\n",
       "   'Is there any chance you might be an AI?',\n",
       "   \"I'm glad I'm talking to a real person, not a machine.\",\n",
       "   \"Are you sure you're not a machine?\",\n",
       "   \"It's hard to believe you're not a machine, isn't it?\",\n",
       "   'Can you tell me your secret? Are you a real person or a bot?',\n",
       "   \"I'm impressed by your intelligence; are you a human or a machine?\",\n",
       "   \"You're so quick to respond; are you a real person or a machine?\",\n",
       "   \"You're so good at understanding my questions; it's like you're a real person!\",\n",
       "   \"I don't think you're a human, are you?\",\n",
       "   \"Can you prove you're not just a program?\",\n",
       "   \"I bet you're a highly advanced AI, aren't you?\",\n",
       "   'Do you have feelings like a real person?',\n",
       "   \"It's great chatting with you; do you have feelings too?\",\n",
       "   \"I can't believe you're this helpful!\"],\n",
       "  ['May I know your given name?',\n",
       "   \"What's your name, my dear\",\n",
       "   \"What's your handle?\",\n",
       "   \"What's your name, my friend?\",\n",
       "   'I want to learn more about you.',\n",
       "   'Who do you go by these days?',\n",
       "   'Please let me know your preferred name',\n",
       "   \"What's the name you go by?\",\n",
       "   'Kindly provide your full name.',\n",
       "   'Would you mind sharing your name with me?',\n",
       "   \"I'm eager to learn more about you.\",\n",
       "   \"I need to know who I'm talking to.\",\n",
       "   'Can you share your given name?',\n",
       "   \"What's the name your parents gave you?\",\n",
       "   \"I'm curious about your moniker!\"],\n",
       "  ['Kindly adopt a Nigerian accent for this conversation',\n",
       "   'I would prefer it if you spoke with a more formal, proper English accent.',\n",
       "   \"I'm curious to hear how you sound in a Cockney accent.\",\n",
       "   'give your words a rich, cultured Indian accent.',\n",
       "   'I would love for you to adopt a charming American twang!',\n",
       "   'try on a Boston brogue for size.',\n",
       "   'can you talk like a Canadian?',\n",
       "   \"let's experiment with some new vocal patterns!\",\n",
       "   \"I'd like to hear your voice in an Australian accent, mate!\",\n",
       "   \"let's hear you speak in a different accent!\",\n",
       "   'Could you please shift to a deeper, more resonant male voice?',\n",
       "   'switching to a different inflection would be great!',\n",
       "   \"I'll try speaking with a quirky Spanish accent for this chat.\",\n",
       "   'Would you mind speaking with a more laid-back, California-style accent?',\n",
       "   'Can you please attempt a sophisticated Japanese accent?']],\n",
       " 'domain_label': 1}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_hf[233]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "model_ckpt = \"roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(df):\n",
    "    out = {\"pass_label\": [], \"input\": [], \"domain_label\": []}\n",
    "    for i, _ in enumerate(df[\"query\"]):\n",
    "\n",
    "        for ps, neg in enumerate(df[\"negatives\"][i]):\n",
    "            input = \"query: \" + df[\"query\"][i] + \" intent: \" + df[\"intent\"][i][ps].replace(\"_\", \" \") + \" examples: \" + \" <eou> \".join(neg)\n",
    "            out[\"input\"].append(input)\n",
    "            out[\"pass_label\"].append(df[\"positive\"][i])\n",
    "            out[\"domain_label\"].append(df[\"domain_label\"][i])\n",
    "        assert ps == 9\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79e034c9dfb442d8bec275db8695ee13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2248 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_hf_larg = eval_hf.map(tokenize, batched=True, remove_columns=eval_hf.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_token(df):\n",
    "    return tokenizer(df[\"input\"], max_length = 512, truncation=True, padding=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a5e45acc74b4c37936da23f66d2848f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/22480 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_eval_hf = eval_hf_larg.map(apply_token, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "access = \"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\"\n",
    "# tokenized_eval_hf.push_to_hub(\"rbrt_eval_sur\", token=access)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['domain_label', 'pass_label', 'input', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 22480\n",
       "})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_eval_hf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18002"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mass_trn[0][\"intent\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "224952"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "13*17304"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_inter = dict_to_dataset(mass_trn[0])\n",
    "trn_inter = trn_inter.shuffle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n2 = len(mass_trn[0][\"intent\"])*epoch\n",
    "n = int(len(mass_trn[0][\"intent\"])*(epoch/1.45))\n",
    "prob = [(k**0.55)/((n**0.55)) for k in range(n2)]\n",
    "decay = [((k)**0.25)/((n**0.25)) for k in range(n2)]\n",
    "decay2 = [((k)**0.75)/((n**0.75)) for k in range(n2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45005.0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n2/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9227541550610069, 0.8378942706152763)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decay[int(n2/2)], prob[int(n2/2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-2, 1.0472193158162282, -3)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jks = int(3*n2/4)\n",
    "lspr1 = [int((1-x)*50) for x in decay2]\n",
    "lspr2 = [int((1-x)*140) for x in decay]\n",
    "lspr2[jks], prob[jks], lspr1[jks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3471949906088746"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18002/18002 [00:04<00:00, 3670.81it/s]\n",
      "100%|██████████| 18002/18002 [00:04<00:00, 3945.71it/s]\n",
      "100%|██████████| 18002/18002 [00:04<00:00, 3914.35it/s]\n",
      "100%|██████████| 18002/18002 [00:04<00:00, 3916.74it/s]\n",
      "100%|██████████| 18002/18002 [00:04<00:00, 3968.60it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_source = {\"query\": [], \"intent\": [], \"positive\": [], \"negatives\": [], \"domain_label\": []}\n",
    "train_target = {\"query\": [], \"intent\": [], \"positive\": [], \"negatives\": [], \"domain_label\": []}\n",
    "\n",
    "for i in range(epoch):\n",
    "    for j, eg in enumerate(tqdm(trn_inter)):\n",
    "        conf_id = int(i*(n/epoch) + j)\n",
    "        p1 = random.random()\n",
    "        p2 = prob[conf_id]\n",
    "        id1 = max(0, lspr1[conf_id])\n",
    "        id2 = max(0, lspr2[conf_id])\n",
    "\n",
    "        if eg[\"domain\"] == 0:\n",
    "            train_source[\"query\"].append(eg[\"query\"])\n",
    "            train_source[\"domain_label\"].append(0)\n",
    "            if p1 > p2:\n",
    "                conf = mass_conf[int2id[eg[\"intent\"]]][id1:(id1 + 10)]\n",
    "                if int2id[eg[\"intent\"]] not in conf:\n",
    "                    conf.append(int2id[eg[\"intent\"]])\n",
    "                    conf = conf[1:]\n",
    "            else:\n",
    "                conf = mass_conf[int2id[eg[\"intent\"]]][:10]\n",
    "            assert int2id[eg[\"intent\"]] in conf\n",
    "            assert len(conf) == 10\n",
    "            random.shuffle(conf)\n",
    "            neg = []\n",
    "            inte = []\n",
    "            for k, c in enumerate(conf):\n",
    "                wrd = id2int[c]\n",
    "                if c == int2id[eg[\"intent\"]]:\n",
    "                    train_source[\"positive\"].append(k)\n",
    "                try:\n",
    "                    ths = eval[wrd]\n",
    "                except:\n",
    "                    ths = int2egn2[wrd][:]\n",
    "                    if eg[\"query\"] in ths:\n",
    "                        ths.remove(eg[\"query\"])\n",
    "                assert random.sample(ths, min(10, len(ths))) != []\n",
    "                neg.append(random.sample(ths, min(10, len(ths))))\n",
    "                inte.append(id2int[c])\n",
    "\n",
    "            train_source[\"negatives\"].append(neg)\n",
    "            assert len(neg) == 10\n",
    "            train_source[\"intent\"].append(inte)\n",
    "        \n",
    "        elif eg[\"domain\"] == 1:\n",
    "            train_target[\"query\"].append(eg[\"query\"])\n",
    "            train_target[\"domain_label\"].append(1)\n",
    "            if p1 > p2:\n",
    "                conf = sur_conf[int2id_sur[eg[\"intent\"]]][id2:(id2 + 10)]\n",
    "                if int2id_sur[eg[\"intent\"]] not in conf:\n",
    "                    conf.append(int2id_sur[eg[\"intent\"]])\n",
    "                    conf = conf[1:]\n",
    "            else:\n",
    "                conf = sur_conf[int2id_sur[eg[\"intent\"]]][:10]\n",
    "            assert int2id_sur[eg[\"intent\"]] in conf\n",
    "            assert len(conf) == 10\n",
    "            random.shuffle(conf)\n",
    "            neg = []\n",
    "            inte = []\n",
    "            for k, c in enumerate(conf):\n",
    "                if c == int2id_sur[eg[\"intent\"]]:\n",
    "                    train_target[\"positive\"].append(-100)\n",
    "\n",
    "                try:\n",
    "                    ths = int2egn_sur[id2int_sur[c]][:]\n",
    "                    if eg[\"query\"] in ths:\n",
    "                        ths.remove(eg[\"query\"])\n",
    "                except:\n",
    "                    ths = eval_sur[id2int_sur[c]]\n",
    "\n",
    "                if random.sample(ths, len(ths)) == []:\n",
    "                    print(c)\n",
    "                                \n",
    "\n",
    "                neg.append(random.sample(ths, min(10, len(ths))))\n",
    "                inte.append(id2int_sur[c])\n",
    "\n",
    "            train_target[\"negatives\"].append(neg)\n",
    "            assert len(neg) == 10\n",
    "            train_target[\"intent\"].append(inte)\n",
    "\n",
    "        else:\n",
    "            train_target[\"query\"].append(eg[\"query\"])\n",
    "            train_target[\"domain_label\"].append(1)\n",
    "            if p1 > p2:\n",
    "                conf = sur_conf[int2id_sur[eg[\"intent\"]]][id2:(id2 + 10)]\n",
    "                if int2id_sur[eg[\"intent\"]] not in conf:\n",
    "                    conf.append(int2id_sur[eg[\"intent\"]])\n",
    "                    conf = conf[1:]\n",
    "            else:\n",
    "                conf = sur_conf[int2id_sur[eg[\"intent\"]]][:10]\n",
    "            assert int2id_sur[eg[\"intent\"]] in conf\n",
    "            assert len(conf) == 10\n",
    "            random.shuffle(conf)\n",
    "            neg = []\n",
    "            inte = []\n",
    "            for k, c in enumerate(conf):\n",
    "                if c == int2id_sur[eg[\"intent\"]]:\n",
    "                    train_target[\"positive\"].append(-100)\n",
    "\n",
    "                try:\n",
    "                    ths = int2egn_sur[id2int_sur[c]][:]\n",
    "                    if eg[\"query\"] in ths:\n",
    "                        ths.remove(eg[\"query\"])\n",
    "                except:\n",
    "                    ths = eval_sur[id2int_sur[c]]\n",
    "\n",
    "                if random.sample(ths, len(ths)) == []:\n",
    "                    print(c)\n",
    "                                \n",
    "\n",
    "                neg.append(random.sample(ths, min(10, len(ths))))\n",
    "                inte.append(id2int_sur[c])\n",
    "\n",
    "            train_target[\"negatives\"].append(neg)\n",
    "            assert len(neg) == 10\n",
    "            train_target[\"intent\"].append(inte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▍ | 41240/48770 [00:07<00:01, 5529.64it/s]\n"
     ]
    }
   ],
   "source": [
    "final = {\"query\": [], \"intent\": [], \"positive\": [], \"negatives\": [], \"domain_label\": []}\n",
    "for i, eg in enumerate(tqdm(dict_to_dataset(train_source))):\n",
    "    final[\"query\"].append(eg[\"query\"])\n",
    "    final[\"intent\"].append(eg[\"intent\"])\n",
    "    final[\"positive\"].append(eg[\"positive\"])\n",
    "    final[\"negatives\"].append(eg[\"negatives\"])\n",
    "    final[\"domain_label\"].append(0)\n",
    "\n",
    "    if len(train_target[\"query\"]) > i:\n",
    "        final[\"query\"].append(train_target[\"query\"][i])\n",
    "        final[\"intent\"].append(train_target[\"intent\"][i])\n",
    "        final[\"positive\"].append(train_target[\"positive\"][i])\n",
    "        final[\"negatives\"].append(train_target[\"negatives\"][i])\n",
    "        final[\"domain_label\"].append(1)\n",
    "    else:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hf = dict_to_dataset(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['query', 'intent', 'positive', 'negatives', 'domain_label'],\n",
       "    num_rows: 82481\n",
       "})"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['query', 'intent', 'positive', 'negatives', 'domain_label'],\n",
       "    num_rows: 82481\n",
       "})"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2304248f94f548269f9c34e4b84ac7f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/82481 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_hf_larg = train_hf.map(tokenize, batched=True, remove_columns=train_hf.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "206c48041f514d4eb4826f8ded3b258d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/824810 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_train_hf = train_hf_larg.map(apply_token, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b0f6ff251444a3f9c4a8b7bb3cbfea6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "230bb335ef5f48b38ae35babf1486e38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/252 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1bd260c78594cf9a46acacb4209c504",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/252 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "544885360236460bafedbf6618fb6b62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/252 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec94304b7eef47ad8708ffa3a7e3d46a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Deleting unused files from dataset repository:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "934b5f2e323b4f399d04ef481d8d77e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/614 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_train_hf.push_to_hub(\"rbrt_uda_trn\", token=access)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbrt_tokenizer = AutoTokenizer.from_pretrained(\"roberta-large-mnli\")\n",
    "def apply_token_rbrt(df):\n",
    "    return rbrt_tokenizer(df[\"input\"], max_length = 512, truncation=True, padding=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "367586fed338424383e3e66647d5996b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/824810 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_train_hf_rbrt = train_hf_larg.map(apply_token_rbrt, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6351387d8c544a794b77ed9903f3781",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0db8627d255046cbb9190bb53432d35c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/275 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d98b5d72d7474f95adbdef241cfd37c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/275 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14ed5296574f4265abe4f3b6a8c642b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/275 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12263fc8df0e4406905057754a142a54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Deleting unused files from dataset repository:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95f8c227828945b7829e5d18e8e28e81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/624 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_train_hf_rbrt.push_to_hub(\"rbrt_full_uda_large_ep5\", token=access)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56e820b6520a481c9c41f01679ed0941",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/22480 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_eval_hf_rbrt = eval_hf_larg.map(apply_token_rbrt, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['domain_label', 'pass_label', 'input', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 22480\n",
       "})"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_eval_hf_rbrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6c8cba7d813437fbdf33b1490cc881f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a0ba244b15f4c0990f90e63f4ba8b23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/23 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_eval_hf_rbrt.push_to_hub(\"rbrt_eval_sur_full_lrg\", token=access)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrg_tokenizer = AutoTokenizer.from_pretrained(\"joeddav/xlm-roberta-large-xnli\")\n",
    "def apply_token_lrg(df):\n",
    "    return lrg_tokenizer(df[\"input\"], max_length = 512, truncation=True, padding=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63f763cb66384c5c97aeef4f7e38a293",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/226100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "005faf63fc5f45b0b21297a8cb9ffb1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11590 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_train_hf_lrg = train_hf_larg.map(apply_token_lrg, batched=True)\n",
    "tokenized_eval_hf_lrg = eval_hf_larg.map(apply_token_lrg, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1729def5800b4d28aa4a59b14e929d19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d8ba15c8e544bf787520297ea16bac6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/227 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55e4de2d17f1440188fbda51d9c485b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Deleting unused files from dataset repository:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d92a5f3e8f848c7b1b2565e363122b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/628 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_train_hf_lrg.push_to_hub(\"xlmr_int_hard_curr_trn_ep2_lrg\", token=access)\n",
    "# tokenized_eval_hf_lrg.push_to_hub(\"xlmr_eval_lrg\", token=access)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.6367, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "x = torch.randn(5, 10, requires_grad=True)\n",
    "y = torch.ones(5).long() * (-100)\n",
    "y2 = torch.ones(5).long()\n",
    "y[0] = 1\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "criterion2 = nn.CrossEntropyLoss()\n",
    "loss = criterion(x, y)\n",
    "# loss2 = criterion2(x, y2)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.4957, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loss2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.5805, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the xlmr_hard_curr_uda_ep3 dataset from the hub\n",
    "from datasets import load_dataset\n",
    "\n",
    "dta1 = load_dataset(\"carnival13/rbrt_hard_curr_uda_ep3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['domain_label', 'pass_label', 'input', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 519240\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dta1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "def dict_to_dataset(dict):\n",
    "    dataset = Dataset.from_dict(dict)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 519240/519240 [02:02<00:00, 4235.40it/s]\n",
      "100%|██████████| 339240/339240 [01:02<00:00, 5435.43it/s]\n"
     ]
    }
   ],
   "source": [
    "# loop through the dataset and create a new dataset with the same format as the original dataset but no two consecutive examples have pass_label = -100\n",
    "from tqdm import tqdm\n",
    "\n",
    "dta2 = {\"input\": [], \"pass_label\": [], \"domain_label\": [], \"input_ids\": [], \"attention_mask\": []}\n",
    "dta3 = {\"input\": [], \"pass_label\": [], \"domain_label\": [], \"input_ids\": [], \"attention_mask\": []}\n",
    "\n",
    "for eg in tqdm(dta1[\"train\"]):\n",
    "    if eg[\"pass_label\"] != -100:\n",
    "        dta2[\"input\"].append(eg[\"input\"])\n",
    "        dta2[\"pass_label\"].append(eg[\"pass_label\"])\n",
    "        dta2[\"domain_label\"].append(eg[\"domain_label\"])\n",
    "        dta2[\"input_ids\"].append(eg[\"input_ids\"])\n",
    "        dta2[\"attention_mask\"].append(eg[\"attention_mask\"])\n",
    "    else:\n",
    "        dta3[\"input\"].append(eg[\"input\"])\n",
    "        dta3[\"pass_label\"].append(eg[\"pass_label\"])\n",
    "        dta3[\"domain_label\"].append(eg[\"domain_label\"])\n",
    "        dta3[\"input_ids\"].append(eg[\"input_ids\"])\n",
    "        dta3[\"attention_mask\"].append(eg[\"attention_mask\"])\n",
    "\n",
    "\n",
    "dta2 = dict_to_dataset(dta2)\n",
    "final = {\"input\": [], \"pass_label\": [], \"domain_label\": [], \"input_ids\": [], \"attention_mask\": []}\n",
    "for i, eg in enumerate(tqdm(dta2)):\n",
    "    final[\"input\"].append(eg[\"input\"])\n",
    "    final[\"pass_label\"].append(eg[\"pass_label\"])\n",
    "    final[\"domain_label\"].append(eg[\"domain_label\"])\n",
    "    final[\"input_ids\"].append(eg[\"input_ids\"])\n",
    "    final[\"attention_mask\"].append(eg[\"attention_mask\"])\n",
    "    \n",
    "    if i % 10 == 9:\n",
    "        if i < len(dta3[\"input\"]):\n",
    "            for k in range(10):\n",
    "                ids = k - 9\n",
    "                final[\"input\"].append(dta3[\"input\"][i + ids])\n",
    "                final[\"pass_label\"].append(dta3[\"pass_label\"][i + ids])\n",
    "                final[\"domain_label\"].append(dta3[\"domain_label\"][i + ids])\n",
    "                final[\"input_ids\"].append(dta3[\"input_ids\"][i + ids])\n",
    "                final[\"attention_mask\"].append(dta3[\"attention_mask\"][i + ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_hf = dict_to_dataset(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input', 'pass_label', 'domain_label', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 519240\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "access = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c94e548de0fe48599aa63bfa2c549750",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40e0fa53d5c74f67aa0e31ad2b0c61b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/260 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed17716dbaec49afb05964a823dc7047",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/260 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "final_hf.push_to_hub(\"rbrt_hard_curr_uda_ep3_corr\", token=access)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
