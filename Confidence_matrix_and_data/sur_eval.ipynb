{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96c1211b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Oct 11 10:17:12 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.191.01   Driver Version: 450.191.01   CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:06:00.0 Off |                    0 |\n",
      "| N/A   33C    P0    60W / 300W |  29704MiB / 32510MiB |      1%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2...  On   | 00000000:07:00.0 Off |                    0 |\n",
      "| N/A   50C    P0    78W / 300W |  19316MiB / 32510MiB |     79%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-SXM2...  On   | 00000000:0A:00.0 Off |                    0 |\n",
      "| N/A   41C    P0    60W / 300W |  31689MiB / 32510MiB |      2%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-SXM2...  On   | 00000000:0B:00.0 Off |                    0 |\n",
      "| N/A   48C    P0    92W / 300W |   7272MiB / 32510MiB |     87%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  Tesla V100-SXM2...  On   | 00000000:85:00.0 Off |                    0 |\n",
      "| N/A   65C    P0   288W / 300W |  24617MiB / 32510MiB |    100%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  Tesla V100-SXM2...  On   | 00000000:86:00.0 Off |                    0 |\n",
      "| N/A   43C    P0    61W / 300W |  11794MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  Tesla V100-SXM2...  On   | 00000000:89:00.0 Off |                    0 |\n",
      "| N/A   46C    P0    64W / 300W |  31688MiB / 32510MiB |      1%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  Tesla V100-SXM2...  On   | 00000000:8A:00.0 Off |                    0 |\n",
      "| N/A   62C    P0   299W / 300W |  17496MiB / 32510MiB |    100%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A    793323      C   python                          29699MiB |\n",
      "|    1   N/A  N/A   3652858      C   python                           7521MiB |\n",
      "|    1   N/A  N/A   3863190      C   python                          11791MiB |\n",
      "|    2   N/A  N/A   3167101      C   python                          31685MiB |\n",
      "|    3   N/A  N/A   3652445      C   python                           7269MiB |\n",
      "|    4   N/A  N/A   3857683      C   ...onda3/envs/wow/bin/python    12823MiB |\n",
      "|    4   N/A  N/A   3861595      C   python                          11791MiB |\n",
      "|    5   N/A  N/A   3864755      C   python                          11791MiB |\n",
      "|    6   N/A  N/A   3169481      C   python                          31681MiB |\n",
      "|    7   N/A  N/A   3856217      C   ...onda3/envs/wow/bin/python    17493MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9aadccab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "610eb137",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d33f2f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6268b2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import XLMRobertaForSequenceClassification, XLMRobertaConfig, XLMRobertaTokenizer, TrainingArguments, Trainer, DataCollatorWithPadding, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers.models.xlm_roberta.modeling_xlm_roberta import XLMRobertaClassificationHead\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import datasets\n",
    "from typing import Optional\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from transformers.trainer import is_datasets_available, seed_worker\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b3f795b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "tokenized_sur = load_dataset(\"carnival13/test_DA_tokenized2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95a30c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ckpt = \"massive_da_eng4/final\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2d532da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReRanker(XLMRobertaForSequenceClassification):\n",
    "    def __init__(self, config: XLMRobertaConfig):\n",
    "        # config.rank_score_index = 32019\n",
    "        config.n_pass = 10\n",
    "        # config.output_hidden_states = True\n",
    "        super().__init__(config)\n",
    "#         self.rank_head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
    "        # self.rank_id = config.rank_score_index\n",
    "        self.n_pass = config.n_pass\n",
    "\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, labels=None, pass_label=None, **kwargs):\n",
    "\n",
    "        batch_size_n, seq_len = input_ids.size()\n",
    "        batch_size = int(batch_size_n/self.n_pass)\n",
    "        labels = None\n",
    "\n",
    "\n",
    "        out = super().forward(input_ids=input_ids, attention_mask=attention_mask, **kwargs)\n",
    "#         rank_score = self.rank_head(out.decoder_hidden_states[-1][:, 0, :])\n",
    "        rank_score = out.logits.view(batch_size, -1)\n",
    "        loss = None\n",
    "\n",
    "\n",
    "        if pass_label != None:\n",
    "            pass_label = pass_label[::self.n_pass]\n",
    "            rank_score = rank_score\n",
    "#             gen_score = out.gpe_score\n",
    "\n",
    "            loss_fct1 = nn.CrossEntropyLoss()\n",
    "#             loss_fct2 = nn.CrossEntropyLoss()\n",
    "\n",
    "            rank_loss = loss_fct1(rank_score, pass_label.view(-1))\n",
    "#             gen_loss = loss_fct2(gen_score, pass_label.view(-1))\n",
    "\n",
    "#             loss = rank_loss + gen_loss\n",
    "\n",
    "            loss = rank_loss\n",
    "            wandb.log({\"loss\": loss})\n",
    "\n",
    "        ret =  SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=out.logits,\n",
    "            hidden_states=out.hidden_states,\n",
    "            attentions=out.attentions\n",
    "        )\n",
    "        ret.rank_score = rank_score\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84d89e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ReRanker.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b344c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93e4622d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import notebook_login\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "466441c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.push_to_hub(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83ae3e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:5\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a8a6ff0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:5'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2de0dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sur.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"pass_label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1264b249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method decode in module transformers.tokenization_utils_base:\n",
      "\n",
      "decode(token_ids: Union[int, List[int], ForwardRef('np.ndarray'), ForwardRef('torch.Tensor'), ForwardRef('tf.Tensor')], skip_special_tokens: bool = False, clean_up_tokenization_spaces: bool = None, **kwargs) -> str method of transformers.models.xlm_roberta.tokenization_xlm_roberta_fast.XLMRobertaTokenizerFast instance\n",
      "    Converts a sequence of ids in a string, using the tokenizer and vocabulary with options to remove special\n",
      "    tokens and clean up tokenization spaces.\n",
      "    \n",
      "    Similar to doing `self.convert_tokens_to_string(self.convert_ids_to_tokens(token_ids))`.\n",
      "    \n",
      "    Args:\n",
      "        token_ids (`Union[int, List[int], np.ndarray, torch.Tensor, tf.Tensor]`):\n",
      "            List of tokenized input ids. Can be obtained using the `__call__` method.\n",
      "        skip_special_tokens (`bool`, *optional*, defaults to `False`):\n",
      "            Whether or not to remove special tokens in the decoding.\n",
      "        clean_up_tokenization_spaces (`bool`, *optional*):\n",
      "            Whether or not to clean up the tokenization spaces. If `None`, will default to\n",
      "            `self.clean_up_tokenization_spaces`.\n",
      "        kwargs (additional keyword arguments, *optional*):\n",
      "            Will be passed to the underlying model specific decode method.\n",
      "    \n",
      "    Returns:\n",
      "        `str`: The decoded sentence.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tokenizer.decode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f4e3db63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"query: Can I make a reservation at Buffalo Wild Wings? intent: apr examples: what is the apr for my capital one card? <sep> How good is the APR on my credit card? <sep> How does the APR on my Wells Fargo card compare to other cards in the market? <sep> Can you please provide me with my credit card's apr details? <sep> What's the APR of my Discover card? <sep> I'm not sure if I understand the difference between apr and interest rate. Can you explain it to me in simple terms? <sep> what is the apr on my wells fargo credit card? <sep> Could you please tell me the apr of my credit card? I want to stay on top of my finances and avoid any surprise charges. <sep> Kindly provide me with my credit card's apr details. <sep> how do I calculate my apr on my discovery card? <sep> Is it possible to lower my credit card's APR <sep> could you please provide me with the apr details of my credit card? <sep> Can you tell me the current APR for my US Bank Cash+ Visa Card? <sep> I would love to learn about the apr on my discover card.\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenized_sur[\"train\"][3][\"input_ids\"], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5705282d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "dl = DataLoader(tokenized_sur[\"train\"], collate_fn=data_collator, batch_size=60, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "00700eef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbe04bce06ac493686dffacd825299d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5598 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "model = model.to(device)\n",
    "batch_len = []\n",
    "labels = []\n",
    "scores = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for b in tqdm(dl):\n",
    "        batch_len.append(int(b[\"input_ids\"].size()[0]))\n",
    "        input_ids = b[\"input_ids\"].to(device)\n",
    "        attention_mask = b[\"attention_mask\"].to(device)\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#         outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        rank_score = list(outputs.logits)\n",
    "        labels += list(b[\"pass_label\"])\n",
    "        scores += rank_score\n",
    "        \n",
    "#         print(pass_label.size())\n",
    "#         mrr.append(cal_mrr(rank_score, pass_label))\n",
    "#         if pred == None:\n",
    "#             pred = rank_score\n",
    "#             lab_p = pass_label\n",
    "#         else:\n",
    "#             pred = torch.cat((pred, rank_score), 0)\n",
    "#             lab_p = torch.cat((lab_p, pass_label), 0)\n",
    "\n",
    "#         for i in r_k.keys():\n",
    "#             r_k[i].append(r_at_k(rank_score, pass_label, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ed72b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = torch.tensor(scores)\n",
    "labels = torch.tensor(labels[::150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51a6a522",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = scores.view(-1, 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "26841358",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2239, 150])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0cdf031f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = torch.argmax(scores, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "58c63557",
   "metadata": {},
   "outputs": [],
   "source": [
    "dct = {\"id\": [], \"intent\": []}\n",
    "for i, j in enumerate(list(prediction)):\n",
    "    dct[\"id\"].append(i)\n",
    "    dct[\"intent\"].append(j)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d7a3679d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30bfb0b8179d492ca93ae6b75e781dea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/2239 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def dict_to_dataset(dict):\n",
    "    dataset = datasets.Dataset.from_dict(dict)\n",
    "    return dataset\n",
    "\n",
    "pred = dict_to_dataset(dct)\n",
    "pred.save_to_disk(f\"pred_xlmr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7251a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "12f91063",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_lst = labels == prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "67ac4d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.8311746120452881\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy = {acc_lst.sum()/2239}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "402b457c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.view(-1, 1) in torch.argsort(scores, dim=1)[:, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bda17a10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([101, 109,   3,  ...,  47,   6,   3])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0273f80e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
