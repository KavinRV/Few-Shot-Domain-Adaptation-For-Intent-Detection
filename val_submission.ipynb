{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_eval = load_dataset(\"carnival13/rbrt_eval_sur_lrg3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['domain_label', 'pass_label', 'input', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 6970\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "sur_df = {\"indoml_id\": [], \"id\": [], \"utt\": [], \"intent\": []}\n",
    "id2ut = {}\n",
    "lst = []\n",
    "\n",
    "# Open the file for reading\n",
    "with open('surprise.data', 'r') as file:\n",
    "    # Iterate through each line in the file\n",
    "    for line in file:\n",
    "        # Parse the JSON data in each line\n",
    "        data = json.loads(line)\n",
    "        assert data[\"id\"] not in id2ut.keys()\n",
    "        lst.append(int(data[\"id\"]))\n",
    "        id2ut[data[\"id\"]] = data[\"utt\"]\n",
    "\n",
    "        for k, v in data.items():\n",
    "            sur_df[k].append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('surprise.solution', 'r') as file:\n",
    "    for i, line in enumerate(file):\n",
    "        data = json.loads(line)\n",
    "        assert lst[i] == int(data[\"indoml_id\"][9:])\n",
    "        # print(data.keys())\n",
    "        sur_df[\"intent\"].append(data[\"intent\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "def dict_to_dataset(dict):\n",
    "    dataset = Dataset.from_dict(dict)\n",
    "    return dataset\n",
    "\n",
    "dataset = dict_to_dataset(sur_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "int2eg = {}\n",
    "for eg in dataset:\n",
    "    try:\n",
    "        if eg[\"utt\"] in int2eg[eg[\"intent\"]]:\n",
    "            continue\n",
    "        int2eg[eg[\"intent\"]].append(eg[\"utt\"])\n",
    "    except:\n",
    "        int2eg[eg[\"intent\"]] = [eg[\"utt\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cook time\n",
      "14\n",
      "distance\n",
      "14\n",
      "insurance\n",
      "14\n",
      "jump start\n",
      "14\n",
      "schedule meeting\n",
      "13\n",
      "time\n",
      "14\n",
      "tire change\n",
      "14\n",
      "tire pressure\n",
      "14\n",
      "yes\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "for k, v in int2eg.items():\n",
    "    if len(v) != 15:\n",
    "        print(k)\n",
    "        print(len(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eg = {\"utt\": [], \"positive\": []}\n",
    "for i, eg in enumerate(tokenized_eval[\"train\"]):\n",
    "    if i%10 == eg[\"pass_label\"]:\n",
    "        test_eg[\"utt\"].append(eg[\"input\"][7:eg[\"input\"].index(\"intent:\")-1])\n",
    "        p = eg[\"input\"][eg[\"input\"].index(\"intent:\") + 8: eg[\"input\"].index(\"examples:\")-1]\n",
    "        test_eg[\"positive\"].append(list(int2eg.keys()).index(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = dict_to_dataset(test_eg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(1==2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['utt', 'positive'],\n",
       "    num_rows: 697\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# samples examples from an intent\n",
    "def few_sample(intent, l):\n",
    "    num = l\n",
    "    few = random.sample(int2eg[intent], num)\n",
    "    random.shuffle(few)\n",
    "    return few"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['utt', 'positive'],\n",
       "    num_rows: 697\n",
       "})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_duplicates(lst):\n",
    "    seen = set()\n",
    "    for item in lst:\n",
    "        if item in seen:\n",
    "            return True\n",
    "        seen.add(item)\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in int2eg.items():\n",
    "    if has_duplicates(v):\n",
    "        print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('surprise_labels.name', 'r') as file:\n",
    "    # Iterate through each line in the file\n",
    "    t = list(l.strip('\\n') for l in file.readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 697/697 [00:00<00:00, 3225.62it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "train_new = {\"query\": [], \"few\": [], \"intent\": [], \"positive\": []}\n",
    "\n",
    "# for each eg in the dataset\n",
    "for eg in tqdm(test):\n",
    "\n",
    "    # add utterance to query\n",
    "    train_new[\"query\"].append(eg[\"utt\"])\n",
    "\n",
    "    negatives = []\n",
    "    for abc in int2eg.values():\n",
    "        a1 = abc[:]\n",
    "        if eg[\"utt\"] in a1:\n",
    "            a1.remove(eg[\"utt\"])\n",
    "        negatives.append(a1)\n",
    "\n",
    "    train_new['intent'].append(list(int2eg.keys()))\n",
    "    # train_new['intent'].append(label2id_dict[eg['intent']])\n",
    "    train_new['few'].append(negatives)\n",
    "    train_new['positive'].append(eg[\"positive\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "# mod_ckp = \"cartesinus/xlm-r-base-amazon-massive-intent-label_smoothing\"\n",
    "mod_ckp = \"roberta-large-mnli\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(mod_ckp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "l = []\n",
    "def tokenize(df):\n",
    "    out = {\"input\": [], \"label\": []}\n",
    "    for i, _ in enumerate(df[\"query\"]):\n",
    "\n",
    "        for ps, neg in enumerate(df[\"few\"][i]):\n",
    "            # print(ty)\n",
    "            input = \"query: \" + df[\"query\"][i] + \" intent: \" + df[\"intent\"][i][ps] + \" examples: \" + \" <eou> \".join(neg)\n",
    "            out[\"input\"].append(input)\n",
    "            out[\"label\"].append(df[\"positive\"][i])\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_dataset = dict_to_dataset(train_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['query', 'few', 'intent', 'positive'],\n",
       "    num_rows: 697\n",
       "})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out = {\"intent\": [], \"few\":[], \"query\": [], \"positive\": []}\n",
    "# for eg1, eg2 in zip(sub_dataset, tokenized_eval[\"train\"][::10]):\n",
    "#     print(eg2)\n",
    "#     out[\"intent\"].append(eg1[\"intent\"])\n",
    "#     p = eg2[\"pass_label\"]\n",
    "#     intes = eg2[\"input\"][eg2[\"input\"].index(\"intent:\")+8: eg2[\"input\"].index(\"examples:\")-1]\n",
    "#     isd = list(int2eg.keys()).index(intes)\n",
    "#     out[\"query\"].append(eg1[\"query\"])\n",
    "#     out[\"few\"].append(eg1[\"few\"])\n",
    "#     out[\"positive\"].append(isd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "486904e2161f411d9a7166d979fa8cd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/697 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sub_dataset_large = sub_dataset.map(tokenize, batched=True, remove_columns=sub_dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input', 'label'],\n",
       "    num_rows: 104550\n",
       "})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_dataset_large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_token(df):\n",
    "    return tokenizer(df[\"input\"], max_length = 512, truncation=True, padding=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62499cf93a484f9ba6f91bfbc91ffd23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/104550 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_sur_dataset = sub_dataset_large.map(apply_token, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "access = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sur_dataset = tokenized_sur_dataset.remove_columns([\"input\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d830344b9fda4028bb0e15df06beffbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca0973993dbe4cd78cb1fa0314d1ab98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/105 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_sur_dataset.push_to_hub(\"rbrt_test_val_lrg3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 104550\n",
       "})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sur_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['query', 'few', 'intent'],\n",
       "    num_rows: 6970\n",
       "})"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['domain_label', 'pass_label', 'input', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 6970\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
